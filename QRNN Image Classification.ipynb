{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ywONg92Q6ngm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-05 00:21:48.471406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-05 00:21:49.736787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from qiskit import *\n",
        "# Qiskit module\n",
        "from qiskit import QuantumCircuit\n",
        "import qiskit.circuit.library as circuit_library\n",
        "import qiskit.quantum_info as qi\n",
        "#from qiskit import execute\n",
        "from qiskit.utils import algorithm_globals\n",
        "from qiskit.circuit.library import EfficientSU2\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
        "from qiskit_machine_learning.connectors import TorchConnector\n",
        "import torch\n",
        "from qiskit.circuit import ParameterVector, Parameter\n",
        "from qiskit.circuit.parametervector import ParameterVectorElement\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tensorflow import compat\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "digits = load_digits()\n",
        "X, y = digits.images[0:100], digits.target[0:100]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "train_mask = np.isin(y_train, [1, 7])\n",
        "X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
        "\n",
        "test_mask = np.isin(y_test, [1, 7])\n",
        "X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 4, 16)\n",
        "X_test = X_test.reshape(X_test.shape[0], 4, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17, 4, 16)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Bgv6jKSGI5ZU"
      },
      "outputs": [],
      "source": [
        "class qrnn(nn.Module):\n",
        "\n",
        "  def __init__(self, n_qrbs, n_qubs, vocab_size, staggered: bool=False):\n",
        "    super().__init__()\n",
        "    self.n_qrbs = n_qrbs\n",
        "    self.n_qubits = n_qubs\n",
        "    self.staggered = staggered\n",
        "    #self.embed_layer = nn.Embedding(vocab_size, n_qrbs, max_norm=1.0)\n",
        "    self.qc_init()\n",
        "    input_params = list(filter(lambda x: not isinstance(x, ParameterVectorElement), self.qc.parameters.data))\n",
        "    weight_params = list(filter(lambda x: isinstance(x, ParameterVectorElement), self.qc.parameters.data))\n",
        "    self.qnn = EstimatorQNN(circuit=self.qc, input_params=input_params, weight_params=weight_params, input_gradients=True)\n",
        "    self.qrnn = TorchConnector(self.qnn)\n",
        "    self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "  def qc_init(self):\n",
        "    self.regD = QuantumRegister(self.n_qubits, 'regD')\n",
        "    self.regH = QuantumRegister(self.n_qubits, 'regH')\n",
        "    self.regY = ClassicalRegister(self.n_qubits, 'regY')\n",
        "    self.qc = QuantumCircuit(self.regD, self.regH, self.regY)\n",
        "    self.theta_matrix = np.array([ParameterVector(f'θ{i}', length=3) for i in range(self.n_qubits*2)])\n",
        "    self.gamma_vec = ParameterVector('γ', length=self.n_qubits*2+1)\n",
        "    self.theta = Parameter('θ')\n",
        "    self.angencode()\n",
        "    self.apply_ansatz()\n",
        "\n",
        "  def angencode(self):\n",
        "    #theta = torch.atan(xt).tolist()[0]\n",
        "    for i in range(self.n_qubits):\n",
        "      self.qc.ry(self.theta, self.regD[i])\n",
        "\n",
        "  def apply_ansatz(self):\n",
        "    qubits = self.qc.qubits\n",
        "\n",
        "    for i in range(self.n_qubits*2):  # Initial circuit rotations with parameters\n",
        "      self.qc.rx(self.theta_matrix[i][0], qubits[i])\n",
        "      self.qc.rz(self.theta_matrix[i][1], qubits[i])\n",
        "      self.qc.rx(self.theta_matrix[i][2], qubits[i])\n",
        "\n",
        "    for i in range(1, self.n_qubits*2):\n",
        "      self.qc.rzz(self.gamma_vec[i], qubits[i-1], qubits[i])\n",
        "\n",
        "    self.qc.rzz(self.gamma_vec[-1], qubits[-1], qubits[0])\n",
        "\n",
        "  def measurement(self):\n",
        "    self.qc.measure(self.regD, self.regY)\n",
        "    # self.qc.reset(self.regD)\n",
        "\n",
        "  def regreset(self):\n",
        "    self.qc.reset(self.regD)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = []\n",
        "    for i in range(self.n_qrbs):\n",
        "      #self.angencode(x[i])\n",
        "      theta = torch.atan(x[i]).view(1)\n",
        "      out = self.softmax(self.qrnn(theta))\n",
        "      y.append(out)\n",
        "      # self.regreset()\n",
        "      # self.measurement()\n",
        "      # self.qc_init()\n",
        "      # y.append(self.regY[0])\n",
        "      # print(y)\n",
        "    return y[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = (torch.round(torch.sign(preds-0.5))+1)//2\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b8aFCz6hvkho"
      },
      "outputs": [],
      "source": [
        "def model_trainer(model, n_epochs, X_train, y_train):\n",
        "  '''\n",
        "  Model trainer to train QRNN\n",
        "  Parameters:\n",
        "    model (PyTorch Model): QRNN model for text or image classification with correct sizes specified\n",
        "    n_epochs (int): Number of epochs to train for.\n",
        "    trainloader (PyTorch Dataloader): Dataloader containing the dataset.\n",
        "  '''\n",
        "  train_loss = []\n",
        "  optimizer = torch.optim.Adam(lr=0.03, params=model.parameters())\n",
        "  criterion = nn.BCELoss()\n",
        "  pbar = tqdm(total=len(X_train), leave=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.train()\n",
        "  model.to(device)\n",
        "  for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    X_tensor=torch.tensor(X_train)\n",
        "    predictions=model(X_tensor.float()).squeeze(1)\n",
        "    #predictions=torch.sign(predictions)\n",
        "    #print(predictions)\n",
        "    label=torch.tensor(y_train)\n",
        "    for i in range(len(label)):\n",
        "        if label[i]==1:\n",
        "            label[i] = 0\n",
        "        else:\n",
        "            label[i]=1\n",
        "    #print(label)\n",
        "    loss = criterion(predictions, label.float())\n",
        "    acc = binary_accuracy(predictions, label)\n",
        "    print('')\n",
        "    print('Accuracy:',acc)\n",
        "    print('')\n",
        "    print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    pbar.update()\n",
        "    pbar.desc = f\"Epoch: {epoch} | Batch: {batch} | Loss {loss}\"\n",
        "    train_loss.append(loss.cpu().detach().numpy())\n",
        "    pbar.refresh()\n",
        "  pbar.close()\n",
        "  return model, train_loss\n",
        "    \n",
        "\n",
        "\n",
        "def model_tester(model, testloader):\n",
        "  '''\n",
        "  Model test to train QRNN\n",
        "  Parameters:\n",
        "    model (PyTorch Model): QRNN model for text or image classification with correct sizes specified\n",
        "    testloader (PyTorch Dataloader): Dataloader containing the test dataset.\n",
        "  '''\n",
        "  preds = []\n",
        "  labels = []\n",
        "  pbar = tqdm(total=len(testloader), leave=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  for batch, (feature, label) in enumerate(testloader):\n",
        "    feature, label = feature.to(device), label.to(device)\n",
        "    with torch.no_grad():\n",
        "      predictions = model(feature.squeeze())\n",
        "      preds.append(predictions.cpu().numpy())\n",
        "      labels.append(label.cpu().numpy())\n",
        "    pbar.update()\n",
        "    pbar.desc = f\"Batch: {batch}\"\n",
        "  pbar.refresh()\n",
        "  pbar.close()\n",
        "  preds = np.array([1 if pred>=0.5 else 0 for pred in preds])\n",
        "  labels = np.array(labels)\n",
        "  acc = (preds == labels).sum() / len(preds)\n",
        "  return preds, acc, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yBMm8lKgiES-"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "660539a6ae114d4188b4ba61171e49f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[1]' is invalid for input of size 64",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining and training the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m qrnn(n_qrbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, n_qubs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, staggered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m trained_model, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mmodel_trainer\u001b[0;34m(model, n_epochs, X_train, y_train)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m X_tensor\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(X_train)\n\u001b[0;32m---> 19\u001b[0m predictions\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#predictions=torch.sign(predictions)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(predictions)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m label\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(y_train)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[15], line 56\u001b[0m, in \u001b[0;36mqrnn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_qrbs):\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;66;03m#self.angencode(x[i])\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m   theta \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqrnn(theta))\n\u001b[1;32m     58\u001b[0m   y\u001b[38;5;241m.\u001b[39mappend(out)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1]' is invalid for input of size 64"
          ]
        }
      ],
      "source": [
        "# Defining and training the model\n",
        "model = qrnn(n_qrbs=16, n_qubs=3, vocab_size=4, staggered=False)\n",
        "trained_model, train_loss = model_trainer(model, 50, X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vukwAYOBYDi"
      },
      "outputs": [],
      "source": [
        "# Calculating accuracy of model\n",
        "preds, accuracy, labels = model_tester(trained_model, imdb_testloader)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2eEguzsEhPd"
      },
      "outputs": [],
      "source": [
        "# Saving model\n",
        "torch.save(model.state_dict(), \"QRNN_STATE.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
